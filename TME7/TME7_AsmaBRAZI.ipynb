{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arftools import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import copy \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def forward(self, y, yhat):\n",
    "        #calcule le cout\n",
    "        pass\n",
    "    \n",
    "    def backward(self, y, yhat):\n",
    "        #calcul le gradient du cout\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        \"\"\" :parameters: stocke les param√®tres du module (la matrice de poids par exemple pour un module lineaire)\n",
    "            :gradient: permet d accumuler le gradient calcule par exemple \n",
    "        \"\"\"\n",
    "        self._parameters=None\n",
    "        self._gradient = None\n",
    "        #delta(j,h-1) c'est les derivees par rapport aux poids de la derniere couche cachee\n",
    "        \n",
    "        #calculer les sorties du module pour les entrees passees en parametre\n",
    "        def forward(self,data):\n",
    "            pass\n",
    "        #reinitialiser a 0 le gradient\n",
    "        def zero_grad(self):\n",
    "            pass\n",
    "        #calculer le gradient du cout par rapport aux parametres et l additionner a la variable gradient\n",
    "        #en fonction de lentree input et des deltas de la couche suivant edelta\n",
    "        def backward_update_gradient(self,inputs,delta):\n",
    "            pass\n",
    "        #calculer le gradient du cout par rapport aux entrees \n",
    "        #en fonction de lentree input et des deltas de la couche suivante delta\n",
    "        def backward_delta(self,inputs,delta):\n",
    "            pass\n",
    "            \n",
    "        #met a jour les parametres du module selon le gradient accumule\n",
    "        #jusqua son appel avec un pas degradient_step    \n",
    "        def update_parameters(self,gradient_step):\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00508948, 0.04439287, 0.96530921]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleLineaire(Module):\n",
    "    def __init__(self, E, S):\n",
    "        self._parameters=np.random.rand(E,S)\n",
    "        self._gradient = None\n",
    "        self.E=E\n",
    "        self.S=S\n",
    "        \n",
    "    def forward(self,data): #data is z h-1\n",
    "        # (M,E) x (E,S)\n",
    "        return np.dot(data.T, self._parameters)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self._gradient = np.zeros((self.E,self.S))\n",
    "    \n",
    "    def backward_update_gradient(self,inputs,delta):\n",
    "        resultat = np.dot(delta, inputs)\n",
    "        self._gradient += resultat\n",
    "\n",
    "    \n",
    "    def update_parameters(self, gradient_step):\n",
    "        self._parameters -= gradient_step * self._gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sig(Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self, inputs):\n",
    "        return 1.0/(1.0+np.exp(-inputs))\n",
    "        \n",
    "    def grad(self, inputs):\n",
    "        return self.sigmoid(inputs)*(1-self.sigmoid(inputs))\n",
    "    \n",
    "    def forward(self, datax):\n",
    "        return self.sigmoid(datax)\n",
    "    \n",
    "    def backward_delta(self, inputs, delta):\n",
    "        return delta * self.grad(inputs)\n",
    "    #we dont update parameters of sigmoid because we dont have parameters so we dont calculate grad by w but \n",
    "    #we need to calculate grad for inputs which if sigma'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
